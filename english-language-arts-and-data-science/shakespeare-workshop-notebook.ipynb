{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    " \n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fpresentations&branch=master&subPath=english-language-arts-and-data-science/shakespeare-workshop-notebook.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare and Statistics\n",
    "\n",
    "![Shakespeare and a chart](https://github.com/callysto/curriculum-notebooks/raw/master/EnglishLanguageArts/ShakespeareStatistics/images/shakespeare-n-stats.png)\n",
    "\n",
    "\n",
    "*Image from https://en.wikipedia.org/wiki/Droeshout_portrait*\n",
    "\n",
    "Can art and science be combined? Natural language processing allows us to use the same statistical skills you might learn in a science class, such as counting up members of a population and looking at their distribution, to gain insight into the written word. Here's an example of what you can do. Let's consider the following question:\n",
    "\n",
    "## What are the top 20 phrases in Shakespeare's Macbeth?\n",
    "\n",
    "Normally when we study Shakespeare we critically read his plays and study the plot, characters, and themes. While this is definitely interesting and useful, we can gain very different insights by taking a multidisciplinary approach.\n",
    "\n",
    "This is something we would probably never want to do if we had to do it by hand. Imagine getting out your clipboard, writing down every different word or phrase you come across and then counting how many times that same word or phrase reappears. Check out how quickly it can be done using Python code in this Jupyter notebook.\n",
    "\n",
    "## Loading the text\n",
    "\n",
    "There are many public domain works available at [Project Gutenberg](http://www.gutenberg.org). You can [search](http://www.gutenberg.org/ebooks) or [browse](http://www.gutenberg.org/catalog) to find works that you are interested in analysing.\n",
    "\n",
    "We are going to search for the play *Macbeth* by William Shakespeare. On the **Download This eBook** page we'll copy the `Plain Text UTF-8` link, then use the `Requests` Python library to download it into a variable called `macbeth`. We can then refer to it by using `macbeth` at any point from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_link = 'http://www.gutenberg.org/files/1533/1533-0.txt'\n",
    "\n",
    "import requests\n",
    "r = requests.get(text_link) # get the online book file\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "macbeth = r.text.split('***')[2] # get the part after the header\n",
    "macbeth = macbeth.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can just print it out to see that we've grabbed the correct document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(macbeth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! But that's a lot of reading to do. And a lot of phrases to count and keep track of. Here's where some Python libraries come into play.\n",
    "\n",
    "## Crunching the text\n",
    "\n",
    "`noun_phrases` will grab groups of words that have been identified as phrases containing nouns. This isn't always 100% correct. English can be a challenging language even for machines, and sometimes the files on [Project Gutenberg](http://www.gutenberg.org) contain errors that make it even harder, but it can usually do a pretty good job.\n",
    "\n",
    "This code cell installs two Python libraries for natural language processing, [textblob](https://textblob.readthedocs.io/en/dev) and [nltk](https://www.nltk.org), then downloads a [corpora data file](http://www.nltk.org/nltk_data) that will allow us to process the text.\n",
    "\n",
    "This may take a while to run. On the left you will see `In [*]:` while it is running. Once it finishes you should see the output printed on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "from textblob import TextBlob\n",
    "macbeth_phrases = TextBlob(macbeth).noun_phrases\n",
    "print(macbeth_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you're seeing is no longer raw text. It's now a list of strings. How long is the list? Let's find out. `len` is short for \"length\", and it will tell you how many items are in any list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(macbeth_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have over 3000 noun phrases. We don't yet know how many of them are repeated.\n",
    "\n",
    "## Counting everything up\n",
    "\n",
    "Here's where this starts to look like a real science project! Let's count the unique phrases and create a table of how many times they occur. They'll be sorted from most to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "unique_texts = list(set(macbeth_phrases))\n",
    "text_counts =  {text: macbeth_phrases.count(text) for text in unique_texts}\n",
    "sorted_texts = sorted(text_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "macbeth_counts = pd.DataFrame(data=sorted_texts, columns=['text', 'count'])\n",
    "macbeth_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of them, so we'll use `.head(20)` which means show the top twenty. In these lists, the first item is always number 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_counts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it! The top 20 phrases in Macbeth! Let's put those in a plot.\n",
    "\n",
    "## Plotting the results\n",
    "\n",
    "You can do almost any kind of plot or other visual representation of observations like this you could think of in Callysto. We'll use the `Plotly Express` library to produce a bar chart, ordered from most to least frequent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "macbeth_counts_sorted = macbeth_counts.head(20).sort_values(by='count', ascending=False)\n",
    "px.bar(macbeth_counts_sorted, x='text', y='count', title='Phrase Frequencies in MacBeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you would prefer a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_counts_sorted = macbeth_counts.head(20).sort_values(by='count', ascending=True)\n",
    "px.bar(macbeth_counts_sorted, y='text', x='count', orientation='h', title='Phrase Frequencies in MacBeth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise, surprise. *Macbeth* is the top phrase in Macbeth. Our main character is mentioned more than twice the number of times as the next most frequent phrase, *Macduff*, and more than three times the frequency that *Lady Macbeth* is mentioned.\n",
    "\n",
    "## Thinking about the results\n",
    "\n",
    "One of the first things we might realize from this simple investigation is the importance of proper nouns. Phrases containing the main characters occur far more frequently than other phrases, and the main character of the play is mentioned far more times than any other characters.\n",
    "\n",
    "Are these observations particular to Macbeth? Or to Shakespeare's plays? Or are they more universal?\n",
    "\n",
    "Now that we've gone through Macbeth, how hard could it be to look at other texts?\n",
    "\n",
    "Let's define a function to to download an ebook from a text url, pull out all the noun phrases, count them up, and plot them. We can then use this for any ebook text that we would like to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_plot(text_url):\n",
    "    r = requests.get(text_url)\n",
    "    r.encoding = 'utf-8'\n",
    "    if 'gutenberg' in text_url:\n",
    "        text = r.text.split('***')[2]\n",
    "    else:\n",
    "        text = r.text\n",
    "    text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "    phrases = TextBlob(text).noun_phrases\n",
    "    unique_texts = list(set(phrases))\n",
    "    text_counts =  {text: phrases.count(text) for text in unique_texts}\n",
    "    sorted_texts = sorted(text_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    counts = pd.DataFrame(data=sorted_texts, columns=['text', 'count'])\n",
    "    global counts_sorted \n",
    "    counts_sorted = counts.head(20).sort_values(by='count', ascending=True)\n",
    "    px.bar(counts_sorted, y='text', x='count', orientation='h', title='Phrase Frequencies').show()\n",
    "print('Word frequency plot function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Hamlet\n",
    "\n",
    "**Hamlet** can be found on [Project Gutenberg](http://www.gutenberg.org) under [EBook-No. 1524](http://www.gutenberg.org/ebooks/1524).\n",
    "\n",
    "Run the following code to download **Hamlet**, pull out all the noun phrases, count them up, and plot them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_plot('http://www.gutenberg.org/files/1524/1524-0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to visualize\n",
    "\n",
    "Bar plots are an excellent way to look at the relative differences in frequencies of the different words, but they're not the only way.\n",
    "\n",
    "Another common way to show the frequencies of words in a text is through a Word Cloud, where the size of each word (or font size) is proportional to the frequency of that word. Similar to a bar plot, the larger words are more frequent in the text used to create them.\n",
    "\n",
    "The below code defines a function that creates a word cloud using the same text as the URL that was given to the bar plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def word_cloud_plot(df):\n",
    "    wc = WordCloud(background_color='white')\n",
    "    df_reindex = df.set_index('text')\n",
    "    freqs = df_reindex['count'].to_dict()\n",
    "    cloud = wc.generate_from_frequencies(frequencies=freqs)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    fig = plt.imshow(cloud)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    fig.axes.spines[['top', 'bottom', 'left', 'right']].set_visible(False)\n",
    "    plt.show()\n",
    "print('Word cloud function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can run the function to generate the word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_plot(counts_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at your own text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at a text of your choice. From [Project Gutenberg](https://www.gutenberg.org), choose a text and click on a plain text file of the text. Copy and paste the link to `\"...\"` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myText = 'https://www.gutenberg.org/cache/epub/67098/pg67098.txt' # Choose your own text to generate a phrase frequencies graph\n",
    "word_frequency_plot(myText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Now let's count how many *noun phrases* your text has, just like we did for Macbeth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = requests.get(myText)\n",
    "r2.encoding = 'utf-8'\n",
    "textfile = r2.text.split('***')[2] \n",
    "textfile = textfile.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') \n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "myText_phrases = TextBlob(textfile).noun_phrases\n",
    "print(myText_phrases)\n",
    "len(myText_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Now that we have a function to plot word frequency, you can try this with other ebooks from [Project Gutenberg](http://www.gutenberg.org). You could also try running this on news articles or any other text, although there may be some tweaks required.\n",
    "\n",
    "However this example is only scratching the surface of natural language processing and text frequency visualization. Libraries such as [spaCy](https://spacy.io) can be used to identify [parts of speech](https://spacy.io/api/annotation#pos-tagging), [vaderSentiment](https://github.com/cjhutto/vaderSentiment) can categorize text based on its tone, and [textstat](https://github.com/shivam5992/textstat) can check the readability and complexity of text. Examples can be found [here](https://github.com/callysto/interesting-problems/blob/main/notebooks/analysing-text-statistics.ipynb).\n",
    "\n",
    "You could use these libraries to compare different writers or different times in history, see if you can trends in sentiment or [word usage](https://books.google.com/ngrams), or investigate changes in language and style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1ca6d17674200220921376aaeb3d36cffe15ecab2470a9a5e7a456cdbf61425"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
